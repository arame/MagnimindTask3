{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task3 - reassesed the requirements, now using dataframes more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "from glob import glob\n",
    "from bs4 import BeautifulSoup\n",
    "import logging, importlib, sys, tqdm\n",
    "import spacy\n",
    "import pickle\n",
    "from _pckle import save_pickle_object, load_pickle_object\n",
    "from _logging import set_logging\n",
    "from _graph import histplot_count, histplot_range_count\n",
    "\n",
    "pkl_df_jobs_file = \"pkl_df_jobs.pkl\"\n",
    "set_logging(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html_file(fname):\n",
    "    with open (fname, 'r', encoding=\"utf8\") as f:\n",
    "        doc = BeautifulSoup(f.read(), 'html.parser')\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_words(text, nlp):\n",
    "    word_tokens = nlp(text)\n",
    "    # converts the words in word_tokens to lower case and then checks whether\n",
    "    # they are present in stop_words or not\n",
    "    tokens = [w for w in word_tokens if not w.is_stop and not w.is_punct and not w.is_space]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_body(tag_body):\n",
    "    body = \" \".join(tag_body.strings)\n",
    "    return body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bullet_points(tag_body):\n",
    "    tag_bullet_point = \"li\"\n",
    "    bullet_points = \"\"\n",
    "    for item in tag_body.find_all(tag_bullet_point):  \n",
    "        bullet_points += item.get_text(separator=\" \")\n",
    "    return bullet_points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resume(file, nlp):\n",
    "    file_path = os.path.join(\"Files\\\\UserInput\", 'resume.txt')\n",
    "    f = open(file_path, \"r\")\n",
    "    resume_text = f.read()\n",
    "    nlp_resume_text = nlp(resume_text)\n",
    "    return resume_text, nlp_resume_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the cosine similarity score between the resume and the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cosine_similarity_score(body, nlp_resume_text, nlp):\n",
    "    nlp_job = nlp(body)\n",
    "    score = nlp_resume_text.similarity(nlp_job)\n",
    "    return score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the job descriptions from the HTML files into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = English()\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "resume_text, nlp_resume_text = get_resume('resume.txt', nlp)\n",
    "files = glob(os.path.join(\"Files\\\\Input\", '*.html'))\n",
    "col_names =  ['Id', 'Title', 'Body', 'Bullet_Points', 'Score']\n",
    "df_jobs = pd.DataFrame(columns = col_names)\n",
    "\n",
    "for i, fname in enumerate(files):\n",
    "    if i % 100 == 0:\n",
    "        logging.info(f\"{i}th file\")\n",
    "    message = f\"{i+1}. file : {fname}\"\n",
    "    logging.info(message)\n",
    "    doc = read_html_file(fname)\n",
    "    title = doc.title.string\n",
    "    body = get_body(doc.body)\n",
    "    bullet_points = get_bullet_points(doc.body)\n",
    "    score = get_cosine_similarity_score(body, nlp_resume_text, nlp)\n",
    "    df_jobs.loc[len(df_jobs)] = [i, title, body, bullet_points, score]\n",
    "\n",
    "\n",
    "save_pickle_object(df_jobs, pkl_df_jobs_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having obtained the dataframe for the job descriptions and stored the data in a pickle file which takes a couple of minutes to run, proceed to the job_selection.ipynb script to determine which job descriptions to use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b41ba045e83f0be7a0a86cbeef029bed6bb1f3047ea5aef815a52ba8b6ba543c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
